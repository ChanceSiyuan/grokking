# Grokking: Empirical Reproduction on Modular Arithmetic

Reproducing the **grokking** phenomenon on modular addition tasks, tracking phase transition diagnostics with five order parameters.

Grokking is a phenomenon where neural networks first memorize training data, then â€” after a long delay â€” suddenly generalize to unseen data. This project implements the full experimental infrastructure described in [Issue #4](https://github.com/ChanceSiyuan/grokking/issues/4).

**ðŸ“– [Read the full notes â†’](https://chancesiyuan.github.io/grokking/)**

- [Survey: Grokking as a Phase Transition](https://chancesiyuan.github.io/grokking/index.html) â€” statistical physics and mean field theory perspective
- [Analytical Proof of Grokking Delay](https://chancesiyuan.github.io/grokking/grokking-delay-proof.html) â€” timescale separation and Kramers barrier analysis

## Setup

Requires Python 3.11+ and [uv](https://docs.astral.sh/uv/).

```bash
uv sync
```

## Quick Start

### Single Experiment

```bash
uv run python scripts/run_single.py \
    --alpha 0.0 \
    --weight-decay 1.0 \
    --n-epochs 30000 \
    --save-dir results/single
```

### Hyperparameter Sweep

Sweep over Î± âˆˆ {0, 0.1, 0.25, 0.5} Ã— Î» âˆˆ {1e-3, 1e-2, 1e-1, 1} (16 runs):

```bash
uv run python scripts/run_sweep.py \
    --n-epochs 100000 \
    --save-dir results/sweep
```

### Regenerate Plots

```bash
uv run python scripts/plot_results.py results/single --type single
uv run python scripts/plot_results.py results/sweep --type sweep
```

## Architecture

- **Model:** 2-layer MLP (Linear â†’ ReLU â†’ Linear), width N=512
- **Task:** Modular addition a + b (mod 97)
- **Data:** 30% of all 97Â² = 9409 pairs for training, 70% for test
- **Optimizer:** AdamW, lr=1e-3
- **Î¼P scaling:** Forward pass scaled by N^{-Î±}, controlling lazy (Î±=0.5) vs rich (Î±=0) regime

## Order Parameters

Five diagnostics tracked during training:

| Parameter | Description |
|-----------|-------------|
| **RQI** | Representation Quality Index â€” embedding geometry regularity |
| **Effective Rank** | Singular value entropy of weight matrix |
| **Participation Ratio** | Active modes in hidden activation covariance |
| **Kernel Alignment** | Alignment between empirical NTK and ideal task kernel |
| **SNR** | Signal-to-noise ratio in Fourier basis |

## Project Structure

```
src/grokking/
â”œâ”€â”€ data.py       # Modular arithmetic dataset
â”œâ”€â”€ model.py      # 2-layer MLP with Î¼P scaling
â”œâ”€â”€ metrics.py    # Order parameter computation
â”œâ”€â”€ train.py      # Training loop
â”œâ”€â”€ viz.py        # Plotting functions
â””â”€â”€ sweep.py      # Hyperparameter sweep
scripts/
â”œâ”€â”€ run_single.py   # Run one experiment
â”œâ”€â”€ run_sweep.py    # Run (Î±, Î») grid
â””â”€â”€ plot_results.py # Regenerate figures
```

## CLI Options

```
usage: run_single.py [-h] [--p P] [--width WIDTH] [--alpha ALPHA]
                     [--weight-decay WEIGHT_DECAY] [--lr LR]
                     [--train-fraction TRAIN_FRACTION] [--n-epochs N_EPOCHS]
                     [--log-every LOG_EVERY] [--save-dir SAVE_DIR]
                     [--seed SEED] [--no-order-params] [--no-plots]
```

## References

- [Power et al. (2022). Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets.](https://arxiv.org/abs/2201.02177)
- [Nanda et al. (2023). Progress Measures for Grokking via Mechanistic Interpretability.](https://arxiv.org/abs/2301.05217)
- [Liu et al. (2022). Omnigrok: Grokking Beyond Algorithmic Data.](https://arxiv.org/abs/2210.01117)
- [Kumar et al. (2024). Grokking as the Transition from Lazy to Rich Training Dynamics.](https://arxiv.org/abs/2310.06110)

## License

MIT
